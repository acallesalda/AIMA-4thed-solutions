{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. \n",
    "\n",
    "Define in your own words: (a) intelligence, (b) artificial intelligence, (c) agent, (d) rationality, (e) logical reasoning.\n",
    "\n",
    "## Answer\n",
    "\n",
    "- __Intelligence__: A mechanism by which an agent manipulated outward information in order to meet a certain goal.\n",
    "- __Artificial Intelligence__: Methods for making machines act intelligently: i.e., make sensible decisions in order to meet cetain goals. This methods are not programmed explicitly in the machine.\n",
    "- __Agent__: An entity which tries to meet cetain goals for a given problem.\n",
    "- __Rationality__: The ability to reflect and give reasons for choosing between different alternatives.\n",
    "- __Logical Reasoning__: A type of reasoning that must follow a given set of rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "\n",
    "Read Turing’s original paper on AI Turing:1950 .In the paper, he discusses several objections to his proposed enterprise and his test for intelligence. Which objections still carry weight? Are his refutations valid? Can you think of new objections arising from developments since he wrote the paper? In the paper, he predicts that, by the year 2000, a computer will have a 30% chance of passing a five-minute Turing Test with an unskilled interrogator. What chance do you think a computer would have today? In another 50 years?\n",
    "\n",
    "# Answer\n",
    "\n",
    "## Objections\n",
    "\n",
    "- The theological objection carries little weight. There is no place for such an objection in modern science. His refutation is kind of weird, but it makes sense as he was trying to refute it theologically also.\n",
    "- The 'heads in the sand' objection. There are still some concerns regarding machines that could achieve human-like intelligence, and their possible dangers. I don't think this is addressed properly by Turing. \n",
    "- The mathematical objection. It is a valid concern, even today, but I think Turing refutes the notion interestingly.\n",
    "- The argument from consciousness. A valid concer, but a bit solipsist as Turing argues. However, we still don't know much about consiousness. This should have been discussed more.\n",
    "- Arguments from various disabilities. I agree with Turing, this are largely arbitrary concern and without much base.\n",
    "- Lady Lovelace objection: This does not carry weight today because of modern methods where you do not program things explicitly, i.e., machine learning.\n",
    "- Argument from continuity in the Nervous system: I agree with Turing, the difference between continuous/discrete does not mean one is intelligent and the other is not.\n",
    "- Argument from informality of behaviour. This is not true, as we can see that many ML models can generalize.\n",
    "- Arguments from extra sensory perception. This absolutely does not hold today. Is Turing being tounge in cheek? Maybe the evidence for ESP back then was different and Turing was misled to believe it was true.\n",
    "\n",
    "## Predictions\n",
    "\n",
    "I think computers now have much greater chances of passing the Turing test, maybe like $40-45\\%$ or maybe nearning $50\\%$. In another $50$ years computers will most probably get very near $50\\%$ of chances for passing the Turing Test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\n",
    "\n",
    "Every year the Loebner Prize is awarded to the program that comes closest to passing a version of the Turing Test. Research and report on the latest winner of the Loebner prize. What techniques does it use? How does it advance the state of the art in AI?\n",
    "\n",
    "## Answer\n",
    "\n",
    "Kuki has won the Loebner prize like $5$ times in a row. It uses a rule based system, improved with reinforcment learning. I think it advances the state of the art but for conversational AI specifically, not the area as a whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. \n",
    "\n",
    "Are reflex actions (such as flinching from a hot stove) rational? Are they intelligent?\n",
    "\n",
    "## Answer\n",
    "\n",
    "They are 'limited-rational' because they are appropiate actions within a limited time. I would call them intelligent, since an AI should be able to take those quick decisions in order to avoid critical harm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\n",
    "\n",
    "There are well-known classes of problems that are intractably difficult for computers, and other classes that are provably undecidable. Does this mean that AI is impossible?\n",
    "\n",
    "## Answer\n",
    "\n",
    "No. For this to be true we would have first to discover how consiousness works algorithmically, and then show that it is an untractable or undecidable problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. \n",
    "\n",
    "Suppose we extend Evans’s SYSTEM program so that it can score 200 on a standard IQ test. Would we then have a program more intelligent than a human? Explain.\n",
    "\n",
    "## Answer\n",
    "\n",
    "No. We would have a program that is good at solving IQ test and nothing more. It would not be useful for anything else. Intelligence implies being able to perform well in a wide variety of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.\n",
    "\n",
    "The neural structure of the sea slug Aplysis has been widely studied (first by Nobel Laureate Eric Kandel) because it has only about 20,000 neurons, most of them large and easily manipulated. Assuming that the cycle time for an Aplysis neuron is roughly the same as for a human neuron, how does the computational power, in terms of memory updates per second, compare with the high-end computer described in (Figure 1.3)?\n",
    "\n",
    "## Answer\n",
    "\n",
    "|                          |                          |\n",
    "| -------------------------|--------------------------|\n",
    "| Computational units      | $20000$ neurons          |\n",
    "| Storage Units            | $20000$ neurons          |\n",
    "|                          | $20'000.000$ synapses    |\n",
    "| Cycle Time               | $10^{-3}$                |\n",
    "| Operations/sec           | $2 * 10^{10}$            |\n",
    "| Memory updates/sec       | $2 * 10^7$               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. \n",
    "\n",
    "How could introspection—reporting on one’s inner thoughts—be inaccurate? Could I be wrong about what I’m thinking? Discuss.\n",
    "\n",
    "## Answer.\n",
    "\n",
    "Introspection can be really inaccurate. We are very emotional beings, and our inner thougths can be greatly biased by those emotions, which may make us see false statements as true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. \n",
    "\n",
    "To what extent are the following computer systems instances of artificial intelligence:\n",
    "- Supermarket bar code scanners.\n",
    "- Web search engines.\n",
    "- Voice-activated telephone menus.\n",
    "- Internet routing algorithms that respond dynamically to the state of the network.\n",
    "\n",
    "## Answer\n",
    "\n",
    "- Supermarket barcode scanner: No artificial intelligence, just different representations of the same thing (the product name).\n",
    "- Websearch engine: Largely uses artificial intelligence to know which pages are more relevant for the query.\n",
    "- Voice-activated telephone menus: Voice recognition is an area of AI so clearly yes.\n",
    "- Internet routing algorithms that respond dynamically to the state of the network: Clearly uses AI, as it responds to changes in the enviroment to make appropriate decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.\n",
    "\n",
    "The question is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. \n",
    "\n",
    "Many of the computational models of cognitive activities that have been proposed involve quite complex mathematical operations, such as convolving an image with a Gaussian or finding a minimum of the entropy function. Most humans (and certainly all animals) never learn this kind of mathematics at all, almost no one learns it before college, and almost no one can compute the convolution of a function with a Gaussian in their head. What sense does it make to say that the “vision system” is doing this kind of mathematics, whereas the actual person has no idea how to do it?\n",
    "\n",
    "## Answer\n",
    "\n",
    "I think we know little about the brain's inner function so we may not actually know how our 'vision system' works algorithmically speaking. So when we are designing computer vision systems, reimplementing our vision system is not feasible, so we just use methods that work in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
